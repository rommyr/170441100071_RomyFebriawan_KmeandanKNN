



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Metode dalam Data Mining">
      
      
        <link rel="canonical" href="https://rommyr.github.io/170441100071_RomyFebriawan_KmeandanKNN/K-Nearest Neighbor/">
      
      
        <meta name="author" content="Romy Febriawan">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>K-Nearest Neighbor - Data Mining</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#k-nearest-neighbor" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://rommyr.github.io/170441100071_RomyFebriawan_KmeandanKNN/" title="Data Mining" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Data Mining
            </span>
            <span class="md-header-nav__topic">
              K-Nearest Neighbor
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://rommyr.github.io/170441100071_RomyFebriawan_KmeandanKNN/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    rommyr/romyfebriawan
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="Pendahuluan" class="md-tabs__link md-tabs__link--active">
        Pendahuluan
      </a>
    
  </li>

      
        
      
        
      
        
      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://rommyr.github.io/170441100071_RomyFebriawan_KmeandanKNN/" title="Data Mining" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Data Mining
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://rommyr.github.io/170441100071_RomyFebriawan_KmeandanKNN/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    rommyr/romyfebriawan
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Pendahuluan" class="md-nav__link">
      Pendahuluan
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../K-Means Clustering/" title="K-Means Clustering" class="md-nav__link">
      K-Means Clustering
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
    <a href="./" title="K-Nearest Neighbor" class="md-nav__link md-nav__link--active">
      K-Nearest Neighbor
    </a>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Decision Tree Classifier/" title="Decision Tree Classifier" class="md-nav__link">
      Decision Tree Classifier
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../software requirements/" title="Software Requirements" class="md-nav__link">
      Software Requirements
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../authors-notes/" title="Author's notes" class="md-nav__link">
      Author's notes
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="k-nearest-neighbor">K-Nearest Neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permanent link">&para;</a></h1>
<h3>Pengertian K-Nearest Neighbor</h3>

<p><img alt="" src="../assets/images/20.png" /></p>
<p>Algoritme <em>k-nearest neighbor</em> (k-NN atau KNN) adalah sebuah metode untuk melakukan <a href="https://id.wikipedia.org/wiki/Pengenalan_pola">klasifikasi</a> terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut.</p>
<p>Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas <em>c</em> jika kelas <em>c</em> merupakan klasifikasi yang paling banyak ditemui pada <em>k</em> buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean.</p>
<p>Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah <em>k</em> buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut.</p>
<p>Nilai <em>k</em> yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai <em>k</em> yang tinggi akan mengurangi efek <em>noise</em> pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai <em>k</em> yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, <em>k</em> = 1) disebut algoritme <em>nearest neighbor</em>.</p>
<p>Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik.</p>
<p>Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya:</p>
<ul>
<li>Linear scan</li>
<li>Pohon kd</li>
<li>Pohon Balltree</li>
<li>Pohon metrik</li>
<li>Locally-sensitive hashing (LSH)</li>
</ul>
<p>Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin <em>error rate</em> yang tidak lebih dari dua kali <em>Bayes error rate</em> (<em>error rate</em> minimum untuk distribusi data tertentu).</p>
<h1 id="kelebihan-dan-kekurangan-knn">Kelebihan dan Kekurangan KNN<a class="headerlink" href="#kelebihan-dan-kekurangan-knn" title="Permanent link">&para;</a></h1>
<p><h4>Kelebihan</h4></p>
<p>KNN memiliki beberapa kelebihan yaitu bahwa dia tangguh terhadap training data yang noisy dan efektif apabila data latih nya besar.</p>
<h4>Kelemahan</h4>

<p>Sedangkan kelemahan dari KNN adalah : </p>
<ol>
<li>KNN perlu menentukan nilai dari parameter K (jumlah dari tetangga terdekat) </li>
<li>Pembelajaran berdasarkan jarak tidak jelas mengenai jenis jarak apa yang harus digunakan dan atribut mana yang harus digunakan untuk mendapatkan hasil yang terbaik </li>
<li>Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari tiap sample uji pada keseluruhan sample latih</li>
</ol>
<h1 id="algoritma-knn">Algoritma KNN<a class="headerlink" href="#algoritma-knn" title="Permanent link">&para;</a></h1>
<h4 id="algoritma-perhitungan-knn">Algoritma Perhitungan KNN<a class="headerlink" href="#algoritma-perhitungan-knn" title="Permanent link">&para;</a></h4>
<ol>
<li>Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru.</li>
<li>Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training.</li>
<li>Urutkan hasil perhitungan tersebut.</li>
<li>Tentukan tetangga terdekat berdasarkan jarak minimum ke K.</li>
<li>Tentukan kategori dari tetangga terdekat dengan objek/data.</li>
<li>Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.</li>
</ol>
<h4 id="contoh-soal-perhitungan-knn">Contoh soal Perhitungan KNN<a class="headerlink" href="#contoh-soal-perhitungan-knn" title="Permanent link">&para;</a></h4>
<p>Diberikan data Training berua dua atribut Bad dan Good untuk mengklasiikasikan sebuah data apakah tergolong Bad atau Good , berikut ini adalah contoh datanya :</p>
<table>
<thead>
<tr>
<th><img alt="" src="assets\images\4.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>contoh data training</td>
</tr>
</tbody>
</table>
<p>Kita diberikan data baru yang akan kita klasifikasikan, yaitu <strong>X = 3</strong> dan <strong>Y = 5</strong>. Jadi termasuk klasifikasi apa data baru ini ? <em>Bad</em> atau <em>Good</em> ?</p>
<h4 id="langkah-penyelesaian">Langkah penyelesaian<a class="headerlink" href="#langkah-penyelesaian" title="Permanent link">&para;</a></h4>
<p><strong>Pertama</strong>, Kita tentukan parameter K. Misalnya kita buat jumlah tertangga terdekat <strong>K = 3</strong>.</p>
<p><strong>Ke-dua</strong>, kita hitung jarak antara data baru dengan semua data training. Kita menggunakan <em>Euclidean Distance</em>. Kita hitung seperti pada table berikut :</p>
<table>
<thead>
<tr>
<th><img alt="" src="assets\images\5.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>perhitungan jarak dengan euclidean distance</td>
</tr>
</tbody>
</table>
<p><strong>Ke-tiga</strong>, kita urutkan jarak dari data baru dengan data training dan menentukan tetangga terdekat berdasarkan jarak minimum K.</p>
<table>
<thead>
<tr>
<th><img alt="" src="assets\images\6.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>pengurutan jarak terdekat data baru dengan data training</td>
</tr>
</tbody>
</table>
<p>Dari kolom 4 (urutan jarak) kita mengurutkan dari yang terdekat ke terjauh antara jarak data baru dengan data training. ada 2 jarak yang sama (yaitu 4) pada data baris 2 dan baris 6, sehingga memiliki urutan yang sama. Pada kolom 5 (Apakah termasuk 3-NN?) maksudnya adalah K-NN menjadi 3-NN , karena nilai K ditentukan sama dengan 3.</p>
<p>Ke-empat</p>
<p>, tentukan kategori dari tetangga terdekat. Kita perhatikan baris 3, 4, dan 5 pada gambar sebelumnya (diatas). Kategori Ya diambil jika nilai</p>
<p>K&lt;=3</p>
<p>. Jadi baris 3, 4, dan 5 termasuk kategori Ya dan sisanya Tidak.</p>
<table>
<thead>
<tr>
<th><img alt="" src="assets\images\7.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>penentuan kategori yang termasuk K=3</td>
</tr>
</tbody>
</table>
<p>Kategori ya untuk K-NN pada kolom 6, mencakup baris 3,4, dan 5. Kita berikan kategori berdasarkan tabel awal. baris 3 memiliki kategori Bad, dan 4,5 memiliki kategori Good.</p>
<p>Ke-lima</p>
<p>, gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi data yang baru.</p>
<table>
<thead>
<tr>
<th><img alt="" src="assets\images\8.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>hasil klasifikasi berdasarkan kategori mayoritas</td>
</tr>
</tbody>
</table>
<p>Data yang kita miliki pada baris 3, 4 dan 5 kita punya 2 kategori Good dan 1 kategori Bad. Dari jumlah mayoritas (</p>
<p>Good &gt; Bad</p>
<p>) tersebut kita simpulkan bahwa data baru (</p>
<p>X=3 dan Y=5</p>
<p>) termasuk dalam kategori</p>
<p>Good</p>
<p>.</p>
<p>Demikian materi mengenai Algoritma K-Nearest Neighbor dan Contoh Soal. </p>
<h1>Implementasi KNN</h1>

<p>Persiapkan data csv</p>
<h3 id="mempersiapkan-dataset">Mempersiapkan Dataset<a class="headerlink" href="#mempersiapkan-dataset" title="Permanent link">&para;</a></h3>
<p>Di tutorial ini, kita bakal pake dataset yang udah populer banget, namanya <a href="https://archive.ics.uci.edu/ml/datasets/Iris">Iris</a>. Ini udah tersedia by default dari <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html">Sklearn</a> nya. Dataset ini isinya tentang 3 macam spesies bunga beserta ukuran petal dan sepal.</p>
<p>Nah, data ini adalah csv, yang kalo dijadikan tabel, tiap barisnya menunjukan berbagai jenis spesies bunga yang berbeda, sedangkan kolomnya menunjukan fitur data yaitu : sepal length, sepal width, petal length, dan petal width secara berurutan (BTW ada 4 kolom ya).</p>
<p>Jenis bunganya ada 3 : setosa, versicolor dan virginica. Ada 50 sampel data tiap jenis bunga nya. jadi kalo di total ada 50x3 sampel data = 150 sampel data.</p>
<p>Oke deh langsung aja, cekidot!</p>
<pre class="codehilite"><code class="language-python"># disini aku bakal import datanya dari sklearn
from sklearn.datasets import load_iris
iris=load_iris()   #datanya di load dulu!
x=iris.data    #fitur data, di print aja kalo mau lihat
y=iris.target   #label data, di print aja kalo mau lihat</code></pre>

<p><em>iris.target_names : buat tahu arti dari iris.target sebenarnya.</em>
<em>iris.feature_names : buat tahu arti dari fitur2 di iris.data</em>
<em>type(iris.data) : kalo mau tahu tipedata nya iris data</em>
<em>iris.data.shape : kalo mau tahu shape atau bentuk metriks dari iris data</em></p>
<p>Menggunakan metode klasifikasi</p>
<pre class="codehilite"><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
import numpy as np
knn=KNeighborsClassifier(n_neighbors=1) #define K=1
knn.fit(x,y)
a=np.array([1.0,2.7,3.6,4.2])
knn.predict(a)</code></pre>

<h3 id="partisi-data">Partisi Data<a class="headerlink" href="#partisi-data" title="Permanent link">&para;</a></h3>
<p>Data yang kita punya kudu dipartisi atau dibagi-bagi jadi train data dan test data. Biasanya ukurannya 8:2 buat train vs test datanya. Di tutorial ini kita bakal tau 2 jenis pembagian data yaitu:</p>
<ol>
<li>Train Test Split</li>
<li>K-fold</li>
</ol>
<h3 id="train-test-data">Train Test Data<a class="headerlink" href="#train-test-data" title="Permanent link">&para;</a></h3>
<p>Yang akan kita lakukan adalah ngebagi kesemua 150 data jadi 2 bagian, data training dan data testing. Perbandingannya bakal otomatis 80:20 persen. Well sebenernya ga pas-pas banget sih.. tapi ya sekitaran itu. Jadi bakal ada x buat training dan testing, begitu juga dengan ‘y’ bakal ada y buat training dan testing. BTW testing itu digunain buat prediksi.</p>
<pre class="codehilite"><code class="language-python">from sklearn.cross_validation import train_test_split
from sklearn import metrics
x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=5)
print(x_train.shape) #buat tau bentuknya x_train (112 baris dan 4 kolom)
print(x_test.shape)  #buat tau bentuknya x_test (38 baris dan 4 kolom)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
print(y_pred)  #hasil prediksi 
print(y_test)  #jawaban yang sebenarnya
print(metrics.accuracy_score(y_test, y_pred))  #score prediksi</code></pre>

<p>Jadi x_train ada 112 data (80-an%) dan x_test ada 38 data (20-an%). 
Terus kita mulai training x_train dan y_train. terus x_test nya di prediksi dan dimasukan ke variabel ‘y_pred’ yang bakal jadi array yang berisi hasil klasifikasi dari ke-38 data dari x_test.</p>
<p>Finally, kita lihat akurasi dari prediksi tadi. Pas ini kita bandingin y_test dan y_pred, karena y_test itu adalah hasil yang seharusya. jadi akurasi disini ngitung sejauh mana sih si ‘y_pred’ ini memenuhi jawaban yang sesungguhnya, yaitu ‘y_test’?</p>
<h3 id="k-fold-cross-validation">K-Fold Cross Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permanent link">&para;</a></h3>
<p>K-fold adalah salah satu metode Cross Validation yang populer dengan melipat data sebanyak K dan mengulangi (men-iterasi) experimennya sebanyak K juga.</p>
<p>Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 lipatan, isinya masing-masing 30 data. Eits, jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan ke-5 lipatan tadi, berarti bakal ada 4 lipatan (kita ganti aja ya nyebutnya jadi partisi ajah) x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data.</p>
<p>Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. Berikut nih gambarannya biar lebih ‘ngeh’.</p>
<p><img alt="" src="E:/DM/New%20folder%20(4)/Template/mkdocs-material-master/docs/assets/images/1.png" /></p>
<p>Source : <a href="https://i.stack.imgur.com/1fXzJ.png">https://i.stack.imgur.com/1fXzJ.png</a></p>
<pre class="codehilite"><code class="language-python">from sklearn.model_selection import KFold
kf=KFold(n_splits=5, shuffle=False)
print(kf)  #buat tau Kfold dan parameter defaultnya
i=1        #ini gapenting, cuma buat nandain fold nya.
for train_index, test_index in kf.split(x):
    print("Fold ", i)
    print("TRAIN :", train_index, "TEST :", test_index)
    x_train=x[train_index]
    x_test=x[test_index]
    y_train=y[train_index]
    y_test=y[test_index]
    i+=1
print("shape x_train :", x_train.shape)
print("shape x_test :", x_test.shape)</code></pre>

<h3 id="cross-validation">Cross Validation<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h3>
<pre class="codehilite"><code>from sklearn.model_selection import cross_val_score
knn= KNeighborsClassifier(n_neighbors=5)
score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') 
print(score)        
print(score.mean())</code></pre>

<p>Disini kita coba implementasi cross_validation yang sesungguhnya. ada 5 parameter di cross_val_score. lo bisa liat lebih detail di dokumentasinya sklearn. Yang jelas ‘cv’=cross validation alias jumlah fold nya. 
Ketika score di print bakal ketauan akurasi tiap iterasi. Kalo kita rata-rata-in, maka scorenya 96%. Sebenernya yang kita code disini lebih praktis dari partisi-partisian tadi. I mean, cross_val_score udah nyedian fungsi partisian sendiri jadi kita gausah pusing kudu ngebagi-bagi data.</p>
<h3 id="tuning-parameter-hyperparameter">Tuning Parameter (Hyperparameter)<a class="headerlink" href="#tuning-parameter-hyperparameter" title="Permanent link">&para;</a></h3>
<pre class="codehilite"><code>k_range = range(1,31)  #1-30
k_score = []
for k in k_range:
    knn=KNeighborsClassifier(n_neighbors=k)
    score= cross_val_score(knn, x, y, cv=10, scoring='accuracy')
    k_score.append(score.mean())
print(k_score)</code></pre>

<p>Kalo ngomongin klasifikasi itu bawaannya seberapa akurat sih klasifikasi data kita? nah di KNN ni punya beberapa parameter yang menentukan tinggi rendahnya akurasi klasifikasi kita. Yaitu jumlah K pada KNN. Kita harus nyobain nih kira2 K=berapa sih yang menghasilkan akurasi paling tinggi? Proses kaya gini nih, nyari parameter yang akurasinya tinggi, namanya Tuning Parameter atau biasanya dibilang juga Finding Hyperparameter.</p>
<p>Disini kita coba 1–30 K. Ntar bakal keliatan K mana yang akurasinya paling jitu.
Semua score pada K yang berbeda2 disimpan di variable k_score. Nah kalo kamu run, hasilnya array yang banyak itu adalah ke 30 score dari K 1–30.</p>
<h3 id="plotting-data">Plotting Data<a class="headerlink" href="#plotting-data" title="Permanent link">&para;</a></h3>
<pre class="codehilite"><code>import matplotlib.pyplot as plt
plt.plot(k_range, k_score)
plt.xlabel('Nomor K')
plt.ylabel('score K')
plt.show()</code></pre>

<p><img alt="" src="E:/DM/New%20folder%20(4)/Template/mkdocs-material-master/docs/assets/images/2.png" /></p>
<p>Ada dua cara yang gue tau buat nge-plot gambar. Pake Matplotlib atau Plotly. Yang pertama ini matplotlib. Tapi gue personally lebih suka plotly sih.. soalnya lebih jelas gt dan cantik hehe.. ya walaupun lebih ribet dikit sih code nya..Sayang disini aku cuma bisa upload png nya. Kalo kamu coba sendiri, kalo kamu hover ke diagramnya, detailnya bakal keliatan gitu jadi lebih akurat. Kamu bisa baca-baca tentang plotly di <a href="https://plot.ly/">sini</a>.</p>
<pre class="codehilite"><code>import plotly.plotly as py
import plotly.graph_objs as go
import plotly.tools as tls
tls.set_credentials_file(username='piyutdyoni', api_key='0fv0PaGxGfWzT72e6S7m')
trace1 = go.Scatter(
                    x=k_range, y=k_score, # Data
                    mode='lines', name='K of KNN' # Additional options
                   )
layout = go.Layout(title='Score of K in KNN',
                   plot_bgcolor='rgb(230)')
fig = go.Figure(data=[trace1], layout=layout)
# Plot data in the notebook
py.iplot(fig, filename='score of K in KNN')</code></pre>

<p><img alt="" src="E:/DM/New%20folder%20(4)/Template/mkdocs-material-master/docs/assets/images/3.png" /></p>
<p>Jadi kedua linechart ini menggambarkan kalo K yang menghasilkan score tertinggi adalah 13, 18, dan 20. Tapi karena batas tertinggi ada di 20, soalnya abis itu scorenya cenderung turun, maka kita ambil pinalty bahwa 20 adalah K yang paling mentok dan menghasilkan score tinggi. Soo… mari kita buktikan apakah 20 adalah jawaban yang benar.</p>
<pre class="codehilite"><code class="language-python">knn=KNeighborsClassifier(n_neighbors=20)
score=cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean()
print(score) #well, hasilnya 98% ^^
</code></pre>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../K-Means Clustering/" title="K-Means Clustering" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                K-Means Clustering
              </span>
            </div>
          </a>
        
        
          <a href="../Decision Tree Classifier/" title="Decision Tree Classifier" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Decision Tree Classifier
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 Romy Febriawan
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="http://struct.cc" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="https://github.com/rommyr" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/rommyr" class="md-footer-social__link fa fa-twitter"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>