{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pendahuluan \u00b6 Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. KNN Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu).","title":"Pendahuluan"},{"location":"#pendahuluan","text":"Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. KNN Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu).","title":"Pendahuluan"},{"location":"Decision Tree Classifier/","text":"K-Nearest Neighbor \u00b6 Pengertian K-Nearest Neighbor Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Kelebihan dan Kekurangan KNN \u00b6 Kelebihan KNN memiliki beberapa kelebihan yaitu bahwa dia tangguh terhadap training data yang noisy dan efektif apabila data latih nya besar. Kelemahan Sedangkan kelemahan dari KNN adalah : KNN perlu menentukan nilai dari parameter K (jumlah dari tetangga terdekat) Pembelajaran berdasarkan jarak tidak jelas mengenai jenis jarak apa yang harus digunakan dan atribut mana yang harus digunakan untuk mendapatkan hasil yang terbaik Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari tiap sample uji pada keseluruhan sample latih Algoritma KNN \u00b6 Algoritma Perhitungan KNN \u00b6 Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru. Contoh soal Perhitungan KNN \u00b6 Diberikan data Training berua dua atribut Bad dan Good untuk mengklasiikasikan sebuah data apakah tergolong Bad atau Good , berikut ini adalah contoh datanya : contoh data training Kita diberikan data baru yang akan kita klasifikasikan, yaitu X = 3 dan Y = 5 . Jadi termasuk klasifikasi apa data baru ini ? Bad atau Good ? Langkah penyelesaian \u00b6 Pertama , Kita tentukan parameter K. Misalnya kita buat jumlah tertangga terdekat K = 3 . Ke-dua , kita hitung jarak antara data baru dengan semua data training. Kita menggunakan Euclidean Distance . Kita hitung seperti pada table berikut : perhitungan jarak dengan euclidean distance Ke-tiga , kita urutkan jarak dari data baru dengan data training dan menentukan tetangga terdekat berdasarkan jarak minimum K. pengurutan jarak terdekat data baru dengan data training Dari kolom 4 (urutan jarak) kita mengurutkan dari yang terdekat ke terjauh antara jarak data baru dengan data training. ada 2 jarak yang sama (yaitu 4) pada data baris 2 dan baris 6, sehingga memiliki urutan yang sama. Pada kolom 5 (Apakah termasuk 3-NN?) maksudnya adalah K-NN menjadi 3-NN , karena nilai K ditentukan sama dengan 3. Ke-empat , tentukan kategori dari tetangga terdekat. Kita perhatikan baris 3, 4, dan 5 pada gambar sebelumnya (diatas). Kategori Ya diambil jika nilai K<=3 . Jadi baris 3, 4, dan 5 termasuk kategori Ya dan sisanya Tidak. penentuan kategori yang termasuk K=3 Kategori ya untuk K-NN pada kolom 6, mencakup baris 3,4, dan 5. Kita berikan kategori berdasarkan tabel awal. baris 3 memiliki kategori Bad, dan 4,5 memiliki kategori Good. Ke-lima , gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi data yang baru. hasil klasifikasi berdasarkan kategori mayoritas Data yang kita miliki pada baris 3, 4 dan 5 kita punya 2 kategori Good dan 1 kategori Bad. Dari jumlah mayoritas ( Good > Bad ) tersebut kita simpulkan bahwa data baru ( X=3 dan Y=5 ) termasuk dalam kategori Good . Demikian materi mengenai Algoritma K-Nearest Neighbor dan Contoh Soal. Implementasi KNN Persiapkan data csv Mempersiapkan Dataset \u00b6 Di tutorial ini, kita bakal pake dataset yang udah populer banget, namanya Iris . Ini udah tersedia by default dari Sklearn nya. Dataset ini isinya tentang 3 macam spesies bunga beserta ukuran petal dan sepal. Nah, data ini adalah csv, yang kalo dijadikan tabel, tiap barisnya menunjukan berbagai jenis spesies bunga yang berbeda, sedangkan kolomnya menunjukan fitur data yaitu : sepal length, sepal width, petal length, dan petal width secara berurutan (BTW ada 4 kolom ya). Jenis bunganya ada 3 : setosa, versicolor dan virginica. Ada 50 sampel data tiap jenis bunga nya. jadi kalo di total ada 50x3 sampel data = 150 sampel data. Oke deh langsung aja, cekidot! # disini aku bakal import datanya dari sklearn from sklearn.datasets import load_iris iris=load_iris() #datanya di load dulu! x=iris.data #fitur data, di print aja kalo mau lihat y=iris.target #label data, di print aja kalo mau lihat iris.target_names : buat tahu arti dari iris.target sebenarnya. iris.feature_names : buat tahu arti dari fitur2 di iris.data type(iris.data) : kalo mau tahu tipedata nya iris data iris.data.shape : kalo mau tahu shape atau bentuk metriks dari iris data Menggunakan metode klasifikasi from sklearn.neighbors import KNeighborsClassifier import numpy as np knn=KNeighborsClassifier(n_neighbors=1) #define K=1 knn.fit(x,y) a=np.array([1.0,2.7,3.6,4.2]) knn.predict(a) Partisi Data \u00b6 Data yang kita punya kudu dipartisi atau dibagi-bagi jadi train data dan test data. Biasanya ukurannya 8:2 buat train vs test datanya. Di tutorial ini kita bakal tau 2 jenis pembagian data yaitu: Train Test Split K-fold Train Test Data \u00b6 Yang akan kita lakukan adalah ngebagi kesemua 150 data jadi 2 bagian, data training dan data testing. Perbandingannya bakal otomatis 80:20 persen. Well sebenernya ga pas-pas banget sih.. tapi ya sekitaran itu. Jadi bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. BTW testing itu digunain buat prediksi. from sklearn.cross_validation import train_test_split from sklearn import metrics x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=5) print(x_train.shape) #buat tau bentuknya x_train (112 baris dan 4 kolom) print(x_test.shape) #buat tau bentuknya x_test (38 baris dan 4 kolom) knn.fit(x_train, y_train) y_pred = knn.predict(x_test) print(y_pred) #hasil prediksi print(y_test) #jawaban yang sebenarnya print(metrics.accuracy_score(y_test, y_pred)) #score prediksi Jadi x_train ada 112 data (80-an%) dan x_test ada 38 data (20-an%). Terus kita mulai training x_train dan y_train. terus x_test nya di prediksi dan dimasukan ke variabel \u2018y_pred\u2019 yang bakal jadi array yang berisi hasil klasifikasi dari ke-38 data dari x_test. Finally, kita lihat akurasi dari prediksi tadi. Pas ini kita bandingin y_test dan y_pred, karena y_test itu adalah hasil yang seharusya. jadi akurasi disini ngitung sejauh mana sih si \u2018y_pred\u2019 ini memenuhi jawaban yang sesungguhnya, yaitu \u2018y_test\u2019? K-Fold Cross Validation \u00b6 K-fold adalah salah satu metode Cross Validation yang populer dengan melipat data sebanyak K dan mengulangi (men-iterasi) experimennya sebanyak K juga. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 lipatan, isinya masing-masing 30 data. Eits, jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan ke-5 lipatan tadi, berarti bakal ada 4 lipatan (kita ganti aja ya nyebutnya jadi partisi ajah) x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. Berikut nih gambarannya biar lebih \u2018ngeh\u2019. Source : https://i.stack.imgur.com/1fXzJ.png from sklearn.model_selection import KFold kf=KFold(n_splits=5, shuffle=False) print(kf) #buat tau Kfold dan parameter defaultnya i=1 #ini gapenting, cuma buat nandain fold nya. for train_index, test_index in kf.split(x): print(\"Fold \", i) print(\"TRAIN :\", train_index, \"TEST :\", test_index) x_train=x[train_index] x_test=x[test_index] y_train=y[train_index] y_test=y[test_index] i+=1 print(\"shape x_train :\", x_train.shape) print(\"shape x_test :\", x_test.shape) Cross Validation \u00b6 from sklearn.model_selection import cross_val_score knn= KNeighborsClassifier(n_neighbors=5) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') print(score) print(score.mean()) Disini kita coba implementasi cross_validation yang sesungguhnya. ada 5 parameter di cross_val_score. lo bisa liat lebih detail di dokumentasinya sklearn. Yang jelas \u2018cv\u2019=cross validation alias jumlah fold nya. Ketika score di print bakal ketauan akurasi tiap iterasi. Kalo kita rata-rata-in, maka scorenya 96%. Sebenernya yang kita code disini lebih praktis dari partisi-partisian tadi. I mean, cross_val_score udah nyedian fungsi partisian sendiri jadi kita gausah pusing kudu ngebagi-bagi data. Tuning Parameter (Hyperparameter) \u00b6 k_range = range(1,31) #1-30 k_score = [] for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') k_score.append(score.mean()) print(k_score) Kalo ngomongin klasifikasi itu bawaannya seberapa akurat sih klasifikasi data kita? nah di KNN ni punya beberapa parameter yang menentukan tinggi rendahnya akurasi klasifikasi kita. Yaitu jumlah K pada KNN. Kita harus nyobain nih kira2 K=berapa sih yang menghasilkan akurasi paling tinggi? Proses kaya gini nih, nyari parameter yang akurasinya tinggi, namanya Tuning Parameter atau biasanya dibilang juga Finding Hyperparameter. Disini kita coba 1\u201330 K. Ntar bakal keliatan K mana yang akurasinya paling jitu. Semua score pada K yang berbeda2 disimpan di variable k_score. Nah kalo kamu run, hasilnya array yang banyak itu adalah ke 30 score dari K 1\u201330. Plotting Data \u00b6 import matplotlib.pyplot as plt plt.plot(k_range, k_score) plt.xlabel('Nomor K') plt.ylabel('score K') plt.show() Ada dua cara yang gue tau buat nge-plot gambar. Pake Matplotlib atau Plotly. Yang pertama ini matplotlib. Tapi gue personally lebih suka plotly sih.. soalnya lebih jelas gt dan cantik hehe.. ya walaupun lebih ribet dikit sih code nya..Sayang disini aku cuma bisa upload png nya. Kalo kamu coba sendiri, kalo kamu hover ke diagramnya, detailnya bakal keliatan gitu jadi lebih akurat. Kamu bisa baca-baca tentang plotly di sini . import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls tls.set_credentials_file(username='piyutdyoni', api_key='0fv0PaGxGfWzT72e6S7m') trace1 = go.Scatter( x=k_range, y=k_score, # Data mode='lines', name='K of KNN' # Additional options ) layout = go.Layout(title='Score of K in KNN', plot_bgcolor='rgb(230)') fig = go.Figure(data=[trace1], layout=layout) # Plot data in the notebook py.iplot(fig, filename='score of K in KNN') Jadi kedua linechart ini menggambarkan kalo K yang menghasilkan score tertinggi adalah 13, 18, dan 20. Tapi karena batas tertinggi ada di 20, soalnya abis itu scorenya cenderung turun, maka kita ambil pinalty bahwa 20 adalah K yang paling mentok dan menghasilkan score tinggi. Soo\u2026 mari kita buktikan apakah 20 adalah jawaban yang benar. knn=KNeighborsClassifier(n_neighbors=20) score=cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean() print(score) #well, hasilnya 98% ^^","title":"Decision Tree Classifier"},{"location":"Decision Tree Classifier/#k-nearest-neighbor","text":"","title":"K-Nearest Neighbor"},{"location":"Decision Tree Classifier/#kelebihan-dan-kekurangan-knn","text":"","title":"Kelebihan dan Kekurangan KNN"},{"location":"Decision Tree Classifier/#algoritma-knn","text":"","title":"Algoritma KNN"},{"location":"Decision Tree Classifier/#algoritma-perhitungan-knn","text":"Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.","title":"Algoritma Perhitungan KNN"},{"location":"Decision Tree Classifier/#contoh-soal-perhitungan-knn","text":"Diberikan data Training berua dua atribut Bad dan Good untuk mengklasiikasikan sebuah data apakah tergolong Bad atau Good , berikut ini adalah contoh datanya : contoh data training Kita diberikan data baru yang akan kita klasifikasikan, yaitu X = 3 dan Y = 5 . Jadi termasuk klasifikasi apa data baru ini ? Bad atau Good ?","title":"Contoh soal Perhitungan KNN"},{"location":"Decision Tree Classifier/#langkah-penyelesaian","text":"Pertama , Kita tentukan parameter K. Misalnya kita buat jumlah tertangga terdekat K = 3 . Ke-dua , kita hitung jarak antara data baru dengan semua data training. Kita menggunakan Euclidean Distance . Kita hitung seperti pada table berikut : perhitungan jarak dengan euclidean distance Ke-tiga , kita urutkan jarak dari data baru dengan data training dan menentukan tetangga terdekat berdasarkan jarak minimum K. pengurutan jarak terdekat data baru dengan data training Dari kolom 4 (urutan jarak) kita mengurutkan dari yang terdekat ke terjauh antara jarak data baru dengan data training. ada 2 jarak yang sama (yaitu 4) pada data baris 2 dan baris 6, sehingga memiliki urutan yang sama. Pada kolom 5 (Apakah termasuk 3-NN?) maksudnya adalah K-NN menjadi 3-NN , karena nilai K ditentukan sama dengan 3. Ke-empat , tentukan kategori dari tetangga terdekat. Kita perhatikan baris 3, 4, dan 5 pada gambar sebelumnya (diatas). Kategori Ya diambil jika nilai K<=3 . Jadi baris 3, 4, dan 5 termasuk kategori Ya dan sisanya Tidak. penentuan kategori yang termasuk K=3 Kategori ya untuk K-NN pada kolom 6, mencakup baris 3,4, dan 5. Kita berikan kategori berdasarkan tabel awal. baris 3 memiliki kategori Bad, dan 4,5 memiliki kategori Good. Ke-lima , gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi data yang baru. hasil klasifikasi berdasarkan kategori mayoritas Data yang kita miliki pada baris 3, 4 dan 5 kita punya 2 kategori Good dan 1 kategori Bad. Dari jumlah mayoritas ( Good > Bad ) tersebut kita simpulkan bahwa data baru ( X=3 dan Y=5 ) termasuk dalam kategori Good . Demikian materi mengenai Algoritma K-Nearest Neighbor dan Contoh Soal.","title":"Langkah penyelesaian"},{"location":"Decision Tree Classifier/#mempersiapkan-dataset","text":"Di tutorial ini, kita bakal pake dataset yang udah populer banget, namanya Iris . Ini udah tersedia by default dari Sklearn nya. Dataset ini isinya tentang 3 macam spesies bunga beserta ukuran petal dan sepal. Nah, data ini adalah csv, yang kalo dijadikan tabel, tiap barisnya menunjukan berbagai jenis spesies bunga yang berbeda, sedangkan kolomnya menunjukan fitur data yaitu : sepal length, sepal width, petal length, dan petal width secara berurutan (BTW ada 4 kolom ya). Jenis bunganya ada 3 : setosa, versicolor dan virginica. Ada 50 sampel data tiap jenis bunga nya. jadi kalo di total ada 50x3 sampel data = 150 sampel data. Oke deh langsung aja, cekidot! # disini aku bakal import datanya dari sklearn from sklearn.datasets import load_iris iris=load_iris() #datanya di load dulu! x=iris.data #fitur data, di print aja kalo mau lihat y=iris.target #label data, di print aja kalo mau lihat iris.target_names : buat tahu arti dari iris.target sebenarnya. iris.feature_names : buat tahu arti dari fitur2 di iris.data type(iris.data) : kalo mau tahu tipedata nya iris data iris.data.shape : kalo mau tahu shape atau bentuk metriks dari iris data Menggunakan metode klasifikasi from sklearn.neighbors import KNeighborsClassifier import numpy as np knn=KNeighborsClassifier(n_neighbors=1) #define K=1 knn.fit(x,y) a=np.array([1.0,2.7,3.6,4.2]) knn.predict(a)","title":"Mempersiapkan Dataset"},{"location":"Decision Tree Classifier/#partisi-data","text":"Data yang kita punya kudu dipartisi atau dibagi-bagi jadi train data dan test data. Biasanya ukurannya 8:2 buat train vs test datanya. Di tutorial ini kita bakal tau 2 jenis pembagian data yaitu: Train Test Split K-fold","title":"Partisi Data"},{"location":"Decision Tree Classifier/#train-test-data","text":"Yang akan kita lakukan adalah ngebagi kesemua 150 data jadi 2 bagian, data training dan data testing. Perbandingannya bakal otomatis 80:20 persen. Well sebenernya ga pas-pas banget sih.. tapi ya sekitaran itu. Jadi bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. BTW testing itu digunain buat prediksi. from sklearn.cross_validation import train_test_split from sklearn import metrics x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=5) print(x_train.shape) #buat tau bentuknya x_train (112 baris dan 4 kolom) print(x_test.shape) #buat tau bentuknya x_test (38 baris dan 4 kolom) knn.fit(x_train, y_train) y_pred = knn.predict(x_test) print(y_pred) #hasil prediksi print(y_test) #jawaban yang sebenarnya print(metrics.accuracy_score(y_test, y_pred)) #score prediksi Jadi x_train ada 112 data (80-an%) dan x_test ada 38 data (20-an%). Terus kita mulai training x_train dan y_train. terus x_test nya di prediksi dan dimasukan ke variabel \u2018y_pred\u2019 yang bakal jadi array yang berisi hasil klasifikasi dari ke-38 data dari x_test. Finally, kita lihat akurasi dari prediksi tadi. Pas ini kita bandingin y_test dan y_pred, karena y_test itu adalah hasil yang seharusya. jadi akurasi disini ngitung sejauh mana sih si \u2018y_pred\u2019 ini memenuhi jawaban yang sesungguhnya, yaitu \u2018y_test\u2019?","title":"Train Test Data"},{"location":"Decision Tree Classifier/#k-fold-cross-validation","text":"K-fold adalah salah satu metode Cross Validation yang populer dengan melipat data sebanyak K dan mengulangi (men-iterasi) experimennya sebanyak K juga. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 lipatan, isinya masing-masing 30 data. Eits, jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan ke-5 lipatan tadi, berarti bakal ada 4 lipatan (kita ganti aja ya nyebutnya jadi partisi ajah) x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. Berikut nih gambarannya biar lebih \u2018ngeh\u2019. Source : https://i.stack.imgur.com/1fXzJ.png from sklearn.model_selection import KFold kf=KFold(n_splits=5, shuffle=False) print(kf) #buat tau Kfold dan parameter defaultnya i=1 #ini gapenting, cuma buat nandain fold nya. for train_index, test_index in kf.split(x): print(\"Fold \", i) print(\"TRAIN :\", train_index, \"TEST :\", test_index) x_train=x[train_index] x_test=x[test_index] y_train=y[train_index] y_test=y[test_index] i+=1 print(\"shape x_train :\", x_train.shape) print(\"shape x_test :\", x_test.shape)","title":"K-Fold Cross Validation"},{"location":"Decision Tree Classifier/#cross-validation","text":"from sklearn.model_selection import cross_val_score knn= KNeighborsClassifier(n_neighbors=5) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') print(score) print(score.mean()) Disini kita coba implementasi cross_validation yang sesungguhnya. ada 5 parameter di cross_val_score. lo bisa liat lebih detail di dokumentasinya sklearn. Yang jelas \u2018cv\u2019=cross validation alias jumlah fold nya. Ketika score di print bakal ketauan akurasi tiap iterasi. Kalo kita rata-rata-in, maka scorenya 96%. Sebenernya yang kita code disini lebih praktis dari partisi-partisian tadi. I mean, cross_val_score udah nyedian fungsi partisian sendiri jadi kita gausah pusing kudu ngebagi-bagi data.","title":"Cross Validation"},{"location":"Decision Tree Classifier/#tuning-parameter-hyperparameter","text":"k_range = range(1,31) #1-30 k_score = [] for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') k_score.append(score.mean()) print(k_score) Kalo ngomongin klasifikasi itu bawaannya seberapa akurat sih klasifikasi data kita? nah di KNN ni punya beberapa parameter yang menentukan tinggi rendahnya akurasi klasifikasi kita. Yaitu jumlah K pada KNN. Kita harus nyobain nih kira2 K=berapa sih yang menghasilkan akurasi paling tinggi? Proses kaya gini nih, nyari parameter yang akurasinya tinggi, namanya Tuning Parameter atau biasanya dibilang juga Finding Hyperparameter. Disini kita coba 1\u201330 K. Ntar bakal keliatan K mana yang akurasinya paling jitu. Semua score pada K yang berbeda2 disimpan di variable k_score. Nah kalo kamu run, hasilnya array yang banyak itu adalah ke 30 score dari K 1\u201330.","title":"Tuning Parameter (Hyperparameter)"},{"location":"Decision Tree Classifier/#plotting-data","text":"import matplotlib.pyplot as plt plt.plot(k_range, k_score) plt.xlabel('Nomor K') plt.ylabel('score K') plt.show() Ada dua cara yang gue tau buat nge-plot gambar. Pake Matplotlib atau Plotly. Yang pertama ini matplotlib. Tapi gue personally lebih suka plotly sih.. soalnya lebih jelas gt dan cantik hehe.. ya walaupun lebih ribet dikit sih code nya..Sayang disini aku cuma bisa upload png nya. Kalo kamu coba sendiri, kalo kamu hover ke diagramnya, detailnya bakal keliatan gitu jadi lebih akurat. Kamu bisa baca-baca tentang plotly di sini . import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls tls.set_credentials_file(username='piyutdyoni', api_key='0fv0PaGxGfWzT72e6S7m') trace1 = go.Scatter( x=k_range, y=k_score, # Data mode='lines', name='K of KNN' # Additional options ) layout = go.Layout(title='Score of K in KNN', plot_bgcolor='rgb(230)') fig = go.Figure(data=[trace1], layout=layout) # Plot data in the notebook py.iplot(fig, filename='score of K in KNN') Jadi kedua linechart ini menggambarkan kalo K yang menghasilkan score tertinggi adalah 13, 18, dan 20. Tapi karena batas tertinggi ada di 20, soalnya abis itu scorenya cenderung turun, maka kita ambil pinalty bahwa 20 adalah K yang paling mentok dan menghasilkan score tinggi. Soo\u2026 mari kita buktikan apakah 20 adalah jawaban yang benar. knn=KNeighborsClassifier(n_neighbors=20) score=cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean() print(score) #well, hasilnya 98% ^^","title":"Plotting Data"},{"location":"K-Means Clustering/","text":"K-Means Clustering \u00b6 Pengertian K-Means Clustering Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Kelebihan dan Kekurangan K-Means Clustering \u00b6 Kelebihan Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar. Algoritma K-Mean \u00b6 Langkah-langkah algortima K-Means Clustering : \u00b6 Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana \u00b6 Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Implementasi K-Means Clustering \u00b6 Import Library yang dibutuhkan \u00b6 Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape Data Preprocessing \u00b6 Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\") Membuat dataframe baru dan memilihnya \u00b6 Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data Menggunakan Elbow method untuk mencari jumlah cluster \u00b6 Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method: Menghitung K-Mean \u00b6 Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini","title":"K-Means Clustering"},{"location":"K-Means Clustering/#k-means-clustering","text":"","title":"K-Means Clustering"},{"location":"K-Means Clustering/#kelebihan-dan-kekurangan-k-means-clustering","text":"Kelebihan Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar.","title":"Kelebihan dan Kekurangan K-Means Clustering"},{"location":"K-Means Clustering/#algoritma-k-mean","text":"","title":"Algoritma K-Mean"},{"location":"K-Means Clustering/#langkah-langkah-algortima-k-means-clustering","text":"Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Langkah-langkah algortima K-Means Clustering :"},{"location":"K-Means Clustering/#contoh-perhitungan-k-means-sederhana","text":"Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Contoh Perhitungan K-Means Sederhana"},{"location":"K-Means Clustering/#implementasi-k-means-clustering","text":"","title":"Implementasi K-Means Clustering"},{"location":"K-Means Clustering/#import-library-yang-dibutuhkan","text":"Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans","title":"Import Library yang dibutuhkan"},{"location":"K-Means Clustering/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape","title":"Memuat Dataset"},{"location":"K-Means Clustering/#data-preprocessing","text":"Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\")","title":"Data Preprocessing"},{"location":"K-Means Clustering/#membuat-dataframe-baru-dan-memilihnya","text":"Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data","title":"Membuat dataframe baru dan memilihnya"},{"location":"K-Means Clustering/#menggunakan-elbow-method-untuk-mencari-jumlah-cluster","text":"Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method:","title":"Menggunakan Elbow method untuk mencari jumlah cluster"},{"location":"K-Means Clustering/#menghitung-k-mean","text":"Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini","title":"Menghitung K-Mean"},{"location":"K-Nearest Neighbor/","text":"K-Nearest Neighbor \u00b6 Pengertian K-Nearest Neighbor Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Kelebihan dan Kekurangan KNN \u00b6 Kelebihan KNN memiliki beberapa kelebihan yaitu bahwa dia tangguh terhadap training data yang noisy dan efektif apabila data latih nya besar. Kelemahan Sedangkan kelemahan dari KNN adalah : KNN perlu menentukan nilai dari parameter K (jumlah dari tetangga terdekat) Pembelajaran berdasarkan jarak tidak jelas mengenai jenis jarak apa yang harus digunakan dan atribut mana yang harus digunakan untuk mendapatkan hasil yang terbaik Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari tiap sample uji pada keseluruhan sample latih Algoritma KNN \u00b6 Algoritma Perhitungan KNN \u00b6 Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru. Contoh soal Perhitungan KNN \u00b6 Diberikan data Training berua dua atribut Bad dan Good untuk mengklasiikasikan sebuah data apakah tergolong Bad atau Good , berikut ini adalah contoh datanya : contoh data training Kita diberikan data baru yang akan kita klasifikasikan, yaitu X = 3 dan Y = 5 . Jadi termasuk klasifikasi apa data baru ini ? Bad atau Good ? Langkah penyelesaian \u00b6 Pertama , Kita tentukan parameter K. Misalnya kita buat jumlah tertangga terdekat K = 3 . Ke-dua , kita hitung jarak antara data baru dengan semua data training. Kita menggunakan Euclidean Distance . Kita hitung seperti pada table berikut : perhitungan jarak dengan euclidean distance Ke-tiga , kita urutkan jarak dari data baru dengan data training dan menentukan tetangga terdekat berdasarkan jarak minimum K. pengurutan jarak terdekat data baru dengan data training Dari kolom 4 (urutan jarak) kita mengurutkan dari yang terdekat ke terjauh antara jarak data baru dengan data training. ada 2 jarak yang sama (yaitu 4) pada data baris 2 dan baris 6, sehingga memiliki urutan yang sama. Pada kolom 5 (Apakah termasuk 3-NN?) maksudnya adalah K-NN menjadi 3-NN , karena nilai K ditentukan sama dengan 3. Ke-empat , tentukan kategori dari tetangga terdekat. Kita perhatikan baris 3, 4, dan 5 pada gambar sebelumnya (diatas). Kategori Ya diambil jika nilai K<=3 . Jadi baris 3, 4, dan 5 termasuk kategori Ya dan sisanya Tidak. penentuan kategori yang termasuk K=3 Kategori ya untuk K-NN pada kolom 6, mencakup baris 3,4, dan 5. Kita berikan kategori berdasarkan tabel awal. baris 3 memiliki kategori Bad, dan 4,5 memiliki kategori Good. Ke-lima , gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi data yang baru. hasil klasifikasi berdasarkan kategori mayoritas Data yang kita miliki pada baris 3, 4 dan 5 kita punya 2 kategori Good dan 1 kategori Bad. Dari jumlah mayoritas ( Good > Bad ) tersebut kita simpulkan bahwa data baru ( X=3 dan Y=5 ) termasuk dalam kategori Good . Demikian materi mengenai Algoritma K-Nearest Neighbor dan Contoh Soal. Implementasi KNN Persiapkan data csv Mempersiapkan Dataset \u00b6 Di tutorial ini, kita bakal pake dataset yang udah populer banget, namanya Iris . Ini udah tersedia by default dari Sklearn nya. Dataset ini isinya tentang 3 macam spesies bunga beserta ukuran petal dan sepal. Nah, data ini adalah csv, yang kalo dijadikan tabel, tiap barisnya menunjukan berbagai jenis spesies bunga yang berbeda, sedangkan kolomnya menunjukan fitur data yaitu : sepal length, sepal width, petal length, dan petal width secara berurutan (BTW ada 4 kolom ya). Jenis bunganya ada 3 : setosa, versicolor dan virginica. Ada 50 sampel data tiap jenis bunga nya. jadi kalo di total ada 50x3 sampel data = 150 sampel data. Oke deh langsung aja, cekidot! # disini aku bakal import datanya dari sklearn from sklearn.datasets import load_iris iris=load_iris() #datanya di load dulu! x=iris.data #fitur data, di print aja kalo mau lihat y=iris.target #label data, di print aja kalo mau lihat iris.target_names : buat tahu arti dari iris.target sebenarnya. iris.feature_names : buat tahu arti dari fitur2 di iris.data type(iris.data) : kalo mau tahu tipedata nya iris data iris.data.shape : kalo mau tahu shape atau bentuk metriks dari iris data Menggunakan metode klasifikasi from sklearn.neighbors import KNeighborsClassifier import numpy as np knn=KNeighborsClassifier(n_neighbors=1) #define K=1 knn.fit(x,y) a=np.array([1.0,2.7,3.6,4.2]) knn.predict(a) Partisi Data \u00b6 Data yang kita punya kudu dipartisi atau dibagi-bagi jadi train data dan test data. Biasanya ukurannya 8:2 buat train vs test datanya. Di tutorial ini kita bakal tau 2 jenis pembagian data yaitu: Train Test Split K-fold Train Test Data \u00b6 Yang akan kita lakukan adalah ngebagi kesemua 150 data jadi 2 bagian, data training dan data testing. Perbandingannya bakal otomatis 80:20 persen. Well sebenernya ga pas-pas banget sih.. tapi ya sekitaran itu. Jadi bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. BTW testing itu digunain buat prediksi. from sklearn.cross_validation import train_test_split from sklearn import metrics x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=5) print(x_train.shape) #buat tau bentuknya x_train (112 baris dan 4 kolom) print(x_test.shape) #buat tau bentuknya x_test (38 baris dan 4 kolom) knn.fit(x_train, y_train) y_pred = knn.predict(x_test) print(y_pred) #hasil prediksi print(y_test) #jawaban yang sebenarnya print(metrics.accuracy_score(y_test, y_pred)) #score prediksi Jadi x_train ada 112 data (80-an%) dan x_test ada 38 data (20-an%). Terus kita mulai training x_train dan y_train. terus x_test nya di prediksi dan dimasukan ke variabel \u2018y_pred\u2019 yang bakal jadi array yang berisi hasil klasifikasi dari ke-38 data dari x_test. Finally, kita lihat akurasi dari prediksi tadi. Pas ini kita bandingin y_test dan y_pred, karena y_test itu adalah hasil yang seharusya. jadi akurasi disini ngitung sejauh mana sih si \u2018y_pred\u2019 ini memenuhi jawaban yang sesungguhnya, yaitu \u2018y_test\u2019? K-Fold Cross Validation \u00b6 K-fold adalah salah satu metode Cross Validation yang populer dengan melipat data sebanyak K dan mengulangi (men-iterasi) experimennya sebanyak K juga. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 lipatan, isinya masing-masing 30 data. Eits, jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan ke-5 lipatan tadi, berarti bakal ada 4 lipatan (kita ganti aja ya nyebutnya jadi partisi ajah) x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. Berikut nih gambarannya biar lebih \u2018ngeh\u2019. Source : https://i.stack.imgur.com/1fXzJ.png from sklearn.model_selection import KFold kf=KFold(n_splits=5, shuffle=False) print(kf) #buat tau Kfold dan parameter defaultnya i=1 #ini gapenting, cuma buat nandain fold nya. for train_index, test_index in kf.split(x): print(\"Fold \", i) print(\"TRAIN :\", train_index, \"TEST :\", test_index) x_train=x[train_index] x_test=x[test_index] y_train=y[train_index] y_test=y[test_index] i+=1 print(\"shape x_train :\", x_train.shape) print(\"shape x_test :\", x_test.shape) Cross Validation \u00b6 from sklearn.model_selection import cross_val_score knn= KNeighborsClassifier(n_neighbors=5) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') print(score) print(score.mean()) Disini kita coba implementasi cross_validation yang sesungguhnya. ada 5 parameter di cross_val_score. lo bisa liat lebih detail di dokumentasinya sklearn. Yang jelas \u2018cv\u2019=cross validation alias jumlah fold nya. Ketika score di print bakal ketauan akurasi tiap iterasi. Kalo kita rata-rata-in, maka scorenya 96%. Sebenernya yang kita code disini lebih praktis dari partisi-partisian tadi. I mean, cross_val_score udah nyedian fungsi partisian sendiri jadi kita gausah pusing kudu ngebagi-bagi data. Tuning Parameter (Hyperparameter) \u00b6 k_range = range(1,31) #1-30 k_score = [] for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') k_score.append(score.mean()) print(k_score) Kalo ngomongin klasifikasi itu bawaannya seberapa akurat sih klasifikasi data kita? nah di KNN ni punya beberapa parameter yang menentukan tinggi rendahnya akurasi klasifikasi kita. Yaitu jumlah K pada KNN. Kita harus nyobain nih kira2 K=berapa sih yang menghasilkan akurasi paling tinggi? Proses kaya gini nih, nyari parameter yang akurasinya tinggi, namanya Tuning Parameter atau biasanya dibilang juga Finding Hyperparameter. Disini kita coba 1\u201330 K. Ntar bakal keliatan K mana yang akurasinya paling jitu. Semua score pada K yang berbeda2 disimpan di variable k_score. Nah kalo kamu run, hasilnya array yang banyak itu adalah ke 30 score dari K 1\u201330. Plotting Data \u00b6 import matplotlib.pyplot as plt plt.plot(k_range, k_score) plt.xlabel('Nomor K') plt.ylabel('score K') plt.show() Ada dua cara yang gue tau buat nge-plot gambar. Pake Matplotlib atau Plotly. Yang pertama ini matplotlib. Tapi gue personally lebih suka plotly sih.. soalnya lebih jelas gt dan cantik hehe.. ya walaupun lebih ribet dikit sih code nya..Sayang disini aku cuma bisa upload png nya. Kalo kamu coba sendiri, kalo kamu hover ke diagramnya, detailnya bakal keliatan gitu jadi lebih akurat. Kamu bisa baca-baca tentang plotly di sini . import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls tls.set_credentials_file(username='piyutdyoni', api_key='0fv0PaGxGfWzT72e6S7m') trace1 = go.Scatter( x=k_range, y=k_score, # Data mode='lines', name='K of KNN' # Additional options ) layout = go.Layout(title='Score of K in KNN', plot_bgcolor='rgb(230)') fig = go.Figure(data=[trace1], layout=layout) # Plot data in the notebook py.iplot(fig, filename='score of K in KNN') Jadi kedua linechart ini menggambarkan kalo K yang menghasilkan score tertinggi adalah 13, 18, dan 20. Tapi karena batas tertinggi ada di 20, soalnya abis itu scorenya cenderung turun, maka kita ambil pinalty bahwa 20 adalah K yang paling mentok dan menghasilkan score tinggi. Soo\u2026 mari kita buktikan apakah 20 adalah jawaban yang benar. knn=KNeighborsClassifier(n_neighbors=20) score=cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean() print(score) #well, hasilnya 98% ^^","title":"K-Nearest Neighbor"},{"location":"K-Nearest Neighbor/#k-nearest-neighbor","text":"","title":"K-Nearest Neighbor"},{"location":"K-Nearest Neighbor/#kelebihan-dan-kekurangan-knn","text":"","title":"Kelebihan dan Kekurangan KNN"},{"location":"K-Nearest Neighbor/#algoritma-knn","text":"","title":"Algoritma KNN"},{"location":"K-Nearest Neighbor/#algoritma-perhitungan-knn","text":"Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.","title":"Algoritma Perhitungan KNN"},{"location":"K-Nearest Neighbor/#contoh-soal-perhitungan-knn","text":"Diberikan data Training berua dua atribut Bad dan Good untuk mengklasiikasikan sebuah data apakah tergolong Bad atau Good , berikut ini adalah contoh datanya : contoh data training Kita diberikan data baru yang akan kita klasifikasikan, yaitu X = 3 dan Y = 5 . Jadi termasuk klasifikasi apa data baru ini ? Bad atau Good ?","title":"Contoh soal Perhitungan KNN"},{"location":"K-Nearest Neighbor/#langkah-penyelesaian","text":"Pertama , Kita tentukan parameter K. Misalnya kita buat jumlah tertangga terdekat K = 3 . Ke-dua , kita hitung jarak antara data baru dengan semua data training. Kita menggunakan Euclidean Distance . Kita hitung seperti pada table berikut : perhitungan jarak dengan euclidean distance Ke-tiga , kita urutkan jarak dari data baru dengan data training dan menentukan tetangga terdekat berdasarkan jarak minimum K. pengurutan jarak terdekat data baru dengan data training Dari kolom 4 (urutan jarak) kita mengurutkan dari yang terdekat ke terjauh antara jarak data baru dengan data training. ada 2 jarak yang sama (yaitu 4) pada data baris 2 dan baris 6, sehingga memiliki urutan yang sama. Pada kolom 5 (Apakah termasuk 3-NN?) maksudnya adalah K-NN menjadi 3-NN , karena nilai K ditentukan sama dengan 3. Ke-empat , tentukan kategori dari tetangga terdekat. Kita perhatikan baris 3, 4, dan 5 pada gambar sebelumnya (diatas). Kategori Ya diambil jika nilai K<=3 . Jadi baris 3, 4, dan 5 termasuk kategori Ya dan sisanya Tidak. penentuan kategori yang termasuk K=3 Kategori ya untuk K-NN pada kolom 6, mencakup baris 3,4, dan 5. Kita berikan kategori berdasarkan tabel awal. baris 3 memiliki kategori Bad, dan 4,5 memiliki kategori Good. Ke-lima , gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi data yang baru. hasil klasifikasi berdasarkan kategori mayoritas Data yang kita miliki pada baris 3, 4 dan 5 kita punya 2 kategori Good dan 1 kategori Bad. Dari jumlah mayoritas ( Good > Bad ) tersebut kita simpulkan bahwa data baru ( X=3 dan Y=5 ) termasuk dalam kategori Good . Demikian materi mengenai Algoritma K-Nearest Neighbor dan Contoh Soal.","title":"Langkah penyelesaian"},{"location":"K-Nearest Neighbor/#mempersiapkan-dataset","text":"Di tutorial ini, kita bakal pake dataset yang udah populer banget, namanya Iris . Ini udah tersedia by default dari Sklearn nya. Dataset ini isinya tentang 3 macam spesies bunga beserta ukuran petal dan sepal. Nah, data ini adalah csv, yang kalo dijadikan tabel, tiap barisnya menunjukan berbagai jenis spesies bunga yang berbeda, sedangkan kolomnya menunjukan fitur data yaitu : sepal length, sepal width, petal length, dan petal width secara berurutan (BTW ada 4 kolom ya). Jenis bunganya ada 3 : setosa, versicolor dan virginica. Ada 50 sampel data tiap jenis bunga nya. jadi kalo di total ada 50x3 sampel data = 150 sampel data. Oke deh langsung aja, cekidot! # disini aku bakal import datanya dari sklearn from sklearn.datasets import load_iris iris=load_iris() #datanya di load dulu! x=iris.data #fitur data, di print aja kalo mau lihat y=iris.target #label data, di print aja kalo mau lihat iris.target_names : buat tahu arti dari iris.target sebenarnya. iris.feature_names : buat tahu arti dari fitur2 di iris.data type(iris.data) : kalo mau tahu tipedata nya iris data iris.data.shape : kalo mau tahu shape atau bentuk metriks dari iris data Menggunakan metode klasifikasi from sklearn.neighbors import KNeighborsClassifier import numpy as np knn=KNeighborsClassifier(n_neighbors=1) #define K=1 knn.fit(x,y) a=np.array([1.0,2.7,3.6,4.2]) knn.predict(a)","title":"Mempersiapkan Dataset"},{"location":"K-Nearest Neighbor/#partisi-data","text":"Data yang kita punya kudu dipartisi atau dibagi-bagi jadi train data dan test data. Biasanya ukurannya 8:2 buat train vs test datanya. Di tutorial ini kita bakal tau 2 jenis pembagian data yaitu: Train Test Split K-fold","title":"Partisi Data"},{"location":"K-Nearest Neighbor/#train-test-data","text":"Yang akan kita lakukan adalah ngebagi kesemua 150 data jadi 2 bagian, data training dan data testing. Perbandingannya bakal otomatis 80:20 persen. Well sebenernya ga pas-pas banget sih.. tapi ya sekitaran itu. Jadi bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. BTW testing itu digunain buat prediksi. from sklearn.cross_validation import train_test_split from sklearn import metrics x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=5) print(x_train.shape) #buat tau bentuknya x_train (112 baris dan 4 kolom) print(x_test.shape) #buat tau bentuknya x_test (38 baris dan 4 kolom) knn.fit(x_train, y_train) y_pred = knn.predict(x_test) print(y_pred) #hasil prediksi print(y_test) #jawaban yang sebenarnya print(metrics.accuracy_score(y_test, y_pred)) #score prediksi Jadi x_train ada 112 data (80-an%) dan x_test ada 38 data (20-an%). Terus kita mulai training x_train dan y_train. terus x_test nya di prediksi dan dimasukan ke variabel \u2018y_pred\u2019 yang bakal jadi array yang berisi hasil klasifikasi dari ke-38 data dari x_test. Finally, kita lihat akurasi dari prediksi tadi. Pas ini kita bandingin y_test dan y_pred, karena y_test itu adalah hasil yang seharusya. jadi akurasi disini ngitung sejauh mana sih si \u2018y_pred\u2019 ini memenuhi jawaban yang sesungguhnya, yaitu \u2018y_test\u2019?","title":"Train Test Data"},{"location":"K-Nearest Neighbor/#k-fold-cross-validation","text":"K-fold adalah salah satu metode Cross Validation yang populer dengan melipat data sebanyak K dan mengulangi (men-iterasi) experimennya sebanyak K juga. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 lipatan, isinya masing-masing 30 data. Eits, jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan ke-5 lipatan tadi, berarti bakal ada 4 lipatan (kita ganti aja ya nyebutnya jadi partisi ajah) x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. Berikut nih gambarannya biar lebih \u2018ngeh\u2019. Source : https://i.stack.imgur.com/1fXzJ.png from sklearn.model_selection import KFold kf=KFold(n_splits=5, shuffle=False) print(kf) #buat tau Kfold dan parameter defaultnya i=1 #ini gapenting, cuma buat nandain fold nya. for train_index, test_index in kf.split(x): print(\"Fold \", i) print(\"TRAIN :\", train_index, \"TEST :\", test_index) x_train=x[train_index] x_test=x[test_index] y_train=y[train_index] y_test=y[test_index] i+=1 print(\"shape x_train :\", x_train.shape) print(\"shape x_test :\", x_test.shape)","title":"K-Fold Cross Validation"},{"location":"K-Nearest Neighbor/#cross-validation","text":"from sklearn.model_selection import cross_val_score knn= KNeighborsClassifier(n_neighbors=5) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') print(score) print(score.mean()) Disini kita coba implementasi cross_validation yang sesungguhnya. ada 5 parameter di cross_val_score. lo bisa liat lebih detail di dokumentasinya sklearn. Yang jelas \u2018cv\u2019=cross validation alias jumlah fold nya. Ketika score di print bakal ketauan akurasi tiap iterasi. Kalo kita rata-rata-in, maka scorenya 96%. Sebenernya yang kita code disini lebih praktis dari partisi-partisian tadi. I mean, cross_val_score udah nyedian fungsi partisian sendiri jadi kita gausah pusing kudu ngebagi-bagi data.","title":"Cross Validation"},{"location":"K-Nearest Neighbor/#tuning-parameter-hyperparameter","text":"k_range = range(1,31) #1-30 k_score = [] for k in k_range: knn=KNeighborsClassifier(n_neighbors=k) score= cross_val_score(knn, x, y, cv=10, scoring='accuracy') k_score.append(score.mean()) print(k_score) Kalo ngomongin klasifikasi itu bawaannya seberapa akurat sih klasifikasi data kita? nah di KNN ni punya beberapa parameter yang menentukan tinggi rendahnya akurasi klasifikasi kita. Yaitu jumlah K pada KNN. Kita harus nyobain nih kira2 K=berapa sih yang menghasilkan akurasi paling tinggi? Proses kaya gini nih, nyari parameter yang akurasinya tinggi, namanya Tuning Parameter atau biasanya dibilang juga Finding Hyperparameter. Disini kita coba 1\u201330 K. Ntar bakal keliatan K mana yang akurasinya paling jitu. Semua score pada K yang berbeda2 disimpan di variable k_score. Nah kalo kamu run, hasilnya array yang banyak itu adalah ke 30 score dari K 1\u201330.","title":"Tuning Parameter (Hyperparameter)"},{"location":"K-Nearest Neighbor/#plotting-data","text":"import matplotlib.pyplot as plt plt.plot(k_range, k_score) plt.xlabel('Nomor K') plt.ylabel('score K') plt.show() Ada dua cara yang gue tau buat nge-plot gambar. Pake Matplotlib atau Plotly. Yang pertama ini matplotlib. Tapi gue personally lebih suka plotly sih.. soalnya lebih jelas gt dan cantik hehe.. ya walaupun lebih ribet dikit sih code nya..Sayang disini aku cuma bisa upload png nya. Kalo kamu coba sendiri, kalo kamu hover ke diagramnya, detailnya bakal keliatan gitu jadi lebih akurat. Kamu bisa baca-baca tentang plotly di sini . import plotly.plotly as py import plotly.graph_objs as go import plotly.tools as tls tls.set_credentials_file(username='piyutdyoni', api_key='0fv0PaGxGfWzT72e6S7m') trace1 = go.Scatter( x=k_range, y=k_score, # Data mode='lines', name='K of KNN' # Additional options ) layout = go.Layout(title='Score of K in KNN', plot_bgcolor='rgb(230)') fig = go.Figure(data=[trace1], layout=layout) # Plot data in the notebook py.iplot(fig, filename='score of K in KNN') Jadi kedua linechart ini menggambarkan kalo K yang menghasilkan score tertinggi adalah 13, 18, dan 20. Tapi karena batas tertinggi ada di 20, soalnya abis itu scorenya cenderung turun, maka kita ambil pinalty bahwa 20 adalah K yang paling mentok dan menghasilkan score tinggi. Soo\u2026 mari kita buktikan apakah 20 adalah jawaban yang benar. knn=KNeighborsClassifier(n_neighbors=20) score=cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean() print(score) #well, hasilnya 98% ^^","title":"Plotting Data"},{"location":"authors-notes/","text":"Penyusun Nama : Romy Febriawan NIM : 170441100071 Prodi : Sistem Informasi Jurusan : Teknik Informatika Universitas : UNIVERSITAS TRUNOJOYO MADURA","title":"Author's notes"},{"location":"software requirements/","text":"Software Requirements \u00b6 Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Library Python yang digunakan: \u00b6 Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn","title":"Software Requirements"},{"location":"software requirements/#software-requirements","text":"Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm","title":"Software Requirements"},{"location":"software requirements/#library-python-yang-digunakan","text":"Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn","title":"Library Python yang digunakan:"}]}